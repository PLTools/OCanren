\section{Related Works and Discussion}
\label{sec:discussion}

\colorbox{yellow!20}{\textbf{Part 1: time complexity analysis in LP}}

To our knowledge, this paper is the first attempt of comprehensive time complexity analysis for interleaving search and \mK.
There are a number of separate observations on how certain patterns in relational programming affect performance and a number of rules of thumb based on these observations (described in \cite{WillsThesis}).
Also some papers (for example \cite{GuardedFresh, FloatArithmetics, UniversalQuantification}) tackling specific problems with relational programming use quantitative time measuring for evaluation.
These approaches to time analysis, although sufficient for specific relational solutions, do not provide the general understanding of interleaving search and its cost.

At the same time, complexity analysis was studied extensively in the broader context of logic programming (mainly, for Prolog).
As one important motivation for complexity analysis is granularity control in parallel execution, the main focus is on the automated approaches.

Probably the best known among them is the framework~\cite{CostAnalysisLP} for cost analysis of logic programs (demonstrated on plain Prolog), implemented in the system CASLOG. It uses data dependecy information to estimate the sizes of the arguments and the number of solutions for executed atoms. These estimations are formulated as recursive inequalities (more precisely, as difference equations for upper bounds), which are then automatically solved with known methods. Time and space complexity are expressed using these estimations, variations of these approach provide both upper~\cite{CostAnalysisLP} and lower~\cite{CostAnalysisLPLower} bounds.

An alternative approach is suggested in the framework~\cite{SymbolicAnalysisLP} for symbolic analysis of logic programs (demonstrated in Prolog with cuts). It constructs symbolic evaluation graphs capturing grounding propagation and reduction of recursive calls to previous ones, in the process of construction some heuristic approximations are used. These graphs may look similar to the symbolic schemes described in the \sectionword~\ref{sec:symbolic} at first glance, but there is a principal difference: symbolic graphs capture the whole execution with all invoked calls (using inverse edges to represent cycles with recursive calls), while our schemes capture only the execution inside the body of a specific relation (representing the information about internal calls in terms of denotational semantics). The graphs are then transformed into term rewriting systems, for which the problem of the complexity analyisis is well-studied (specifically, AProVE tool is used).

While these two approaches can be seen as partial bases for our technique, they are mainly focused on how the information about the arguments and results of the evaluated clauses can be derived automatially, since calculation of time complexity is trivial with this information for SLD-resolution. In contrast, we are interested in the penalty of non-trivial scheduling of relational calls under interleaving seach, so we delegate handling the information about calls to the reasoning in a specific metatheory. Should the need arise, these approaches may be adopted to fully automate our technique.

\colorbox{yellow!20}{\textbf{Part 2: ignoring unifications}}

In this paper we analyse only complexity of the scheduling of unifications, ignoring complexity of performing these unifications. However, we argue that the scheduling factor we calculate can be used as an approximation for the whole program execution complexity in most practical cases. This is not unusual: it is common knowledge among users of Prolog that in practise almost always each unification takes constant time (depending on the input), some theoretical basis for this is given in~\cite{UnificationAverageCost}. Therefore it is conventional for complexity analyses of Prolog to use the number of unifications or the number of resolution steps (which are assymptotically equivalent) as the time measure.

There are some specifics of unification implementation in \mK. First, for simple backtracking in a non-mutable fashion the triangular substitutions~\cite{UnificationTheory} are used instead of the usual indempotent ones. It brings additional overhead that is analysed in some detail in~\cite{WillsTheisis}, but experience shows that in most practical cases this overhead in insignificant (for this reason the simplest implementations with bad worst-time complexity are kept). Secondly, \mK allows programmer to chose whether to perform occurs checks in a specific unification, and by default they are performed, which this time adds significant overhead usually changing the resulting complexity. These checks are rarely violated (in the context of Prolog this is established in~\cite{OccursChecksRevisited}), so in other languages (like Prolog) the occurs checks are usually ommited by default. Sheduling factor we compute can be used as a measure for time in \mK only if occurs checks are ommited.

\colorbox{yellow!20}{\textbf{Part 3: limitations}}

Our approach imposes three conditions on the analyzed programs: disjunctive normal form, non-repetiveness of answers and grounding of relational calls. The first two are rather non-restrictive: DNF is equivalent to the description of relation as a set of horn clauses in Prolog and most well-known examples in \mK are written in this or very similar form; repetitive answers are usually an indication of a wrongly written relation~\cite{WillsThesis}. The groundness condition is more serious: it prohibits program execution to collect infinitely many individual ground solutions in one answer using free variables, which is a useful pattern. At the same time, this condition is known (the framework for CASLOG system mentioned above imposes exactly the same condition) and experience shows that many important kinds of programs satisfy it (although it is hard to characterize the class of such programs precisely). Relaxing of any of these restriction will likely mess up the current relatively compact desrciption of symbolic schemes (for the conditions on relational calls) or the form of the extracted inequalities (for the DNF condition).

Also, for now we confine ourselves to the problem of estimating the time of the full search for a given query. Estimating the time before the first (or some specific) answer is found is an important and probably more practical task. Unfortunately, the described technique can not be adjusted naturally to this case. The reason for this is that the reasoning about time (scheduling time in particular) in our terms becomes non-compositional for the case of interrupted search: if the answer is found in some branch, the search is cut short in other branches too. And we still need to analyse every branch, since the height of states can be different in different branches, so the equal number of semantic steps in different branches can take different amounts of time to evaluate. This picture requires more complicated notions with non-trivial dependencies between them and the model becomes impractical. We still can use our technique to establish some rouugh lower and upper bounds (for example via relation between two complexity factors from \lemmaword~\ref{lem:lem:d_t_relation}), but in general this problem requires a separate thorough analysis,

