\section{Scheduling Complexity}
\label{sec:scheduling}

In this section we define a specific value to estimate the scheduling time and derive some equations to calculate this value for a given \emph{semantic
state}. However, our ultimate goal is to provide a complexity estimation for a given goal (\emph{query}). This problem will be addressed in Section~\ref{sec:symbolic},
in which we will make use of recurrent equations presented here.

We may notice that operational semantics described in the previous section can be used to calculate the exact number of elementary scheduling steps.
Our first idea is to take the number of states $d\,(s)$ in the finite trace for a given state $s$:

\[ d\,(s) \; \eqdef \; | \trs{s} |  \]

However, it turns out, that this value alone does not provide an accurate scheduling complexity estimation. The reason is that some
elementary steps in the semantics are not elementary in existing implementations. Namely, a careful analysis discovers that
each semantic step involves a navigation to the leftmost leaf of the state which in implementations corresponds to multiple elementary actions,
which number is proportional to the length of the leftmost branch of the state in question. Here we provide an \emph{ad-hoc} definition for this value, $t\,(s)$,
which we call the \emph{scheduling factor}:

\[
t\,(s) \eqdef \sum\limits_{s_i \in \trs{s}} lh\,(s_i) 
\]

where

\[
\begin{array}{rcl}
 lh\,(\taskst{g}{e})  &\eqdef& 1 \\
 lh\,(s_1 \oplus s_2) &\eqdef& lh\,(s_1) + 1 \\
 lh\,(s \otimes g)    &\eqdef& lh\,(s) + 1 \\
\end{array}
\]

In the rest of the section we state a number of lemmas providing estimations for these two values. The proofs for the lemmas (when omitted) can be
found in Appendix~\ref{sec:appendix}.

The first lemma provides a fundamental relation between these two~--- $d$ and $t$,~--- estimations for the scheduling complexity:

\begin{lemma}
  For any state $s$

  \[
  d\,(s) \le t\,(s) \le d^2\,(s)
  \]
  
\end{lemma}

Our next goal is to derive recurrent equations which would relate the scheduling complexity for states to the scheduling complexity for their
(immediate) substates. It turns out that to come up with such equations both $t$ and $d$ values have to be estimated simultaneously.  


% We take scheduling factor $t\,(s)$ as a value that determines the scheduling complexity $T_s$, but we will also need to calculate $d\,(s)$ as it will be used in the equations for $t\,(s)$. 

The following trivial lemma handles the basic (leaf state) case:

\begin{lemma}
  If

  \[\taskst{g}{e} \xrightarrow{l} s^\prime\]

  then

  \[d\,(\taskst{g}{e}) = d\,(s^\prime) + 1\]

  and

  \[t\,(\taskst{g}{e}) = t\,(s^\prime) + 1\]
\end{lemma}
\begin{proof}
  By a direct inspection of operational semantics rules.\qed
\end{proof}

%In $\oplus$-states the substates are evaluated separately, one step at a time for each substate,
%so the total number of semantic steps is just a sum.
%However, for the scheduling factor, there is an extra summand since the heights of the states in
%the trace becomes bigger (by one additional $\oplus$-node on the top).
%This additional node exists in the trace until one of the substates is evaluated completely, so the
%scheduling factor is increased by the number of steps before such an event.
%So we have the following lemma.

The next lemma provides the equations for $\oplus$-states:

\begin{lemma}
\label{lem:sum_estimation}
For any two states $s_1$ and $s_2$

\[
\begin{array}{rcl}
  d\,(s_1 \oplus s_2) &=& d\,(s_1) + d\,(s_2) \\

%  t\,(s_1 \oplus s_2) &=& t\,(s_1) + t\,(s_2) + \min\,\{2\cdot d\,(s_1) - 1, 2\cdot d\,(s_2)\}
    t\,(s_1 \oplus s_2) &=& t\,(s_1) + t\,(s_2) + \costdisj{s_1}{s_2}
\end{array}
\]

where

\[ \costdisj{s_1}{s_2} = \min\,\{2\cdot d\,(s_1) - 1, 2\cdot d\,(s_2)\} \] 

\end{lemma}

Informally, for a state in the form $s_1 \oplus s_2$ the substates are evaluated separately, one step at a time for
each substate, so the total number of semantic steps is the sum of those for the substates. However, for the scheduling factor, 
there is an extra summand $\costdisj{s_1}{s_2}$ since the ``leftmost heights'' of the states in the trace are one node greater then those for the
original substates due to the introduction of one additional $\oplus$-node on the top. This additional node persists in the trace until the evaluation
of one of the substates comes to an end, so the scheduling factor is increased by the number of steps until that.

The next lemma provides the equations for $\otimes$-states:

\begin{lemma}
For any state $s$ and any goal $g$

\[
\begin{array}{rclr}
d\,(s \otimes g)  &=&  d\,(s) + \smashoperator[lr]{\sum\limits_{a_i \in \tra{s}}} d\,(\taskst{g}{a_i})& \qquad(\star) \\

 t\,(s \otimes g)  &=&  t\,(s) + \costconj{s}{g} + \smashoperator[lr]{\sum\limits_{a_i \in \tra{s}}} (t\,(\taskst{g}{a_i}) + \costdisj{\taskst{g}{a_i}}{(s'_i \otimes g)})&\qquad(\dagger)
\end{array}
\]

where 

\[
\begin{array}{rcl}
\costdisj{s_1}{s_2} & = & \min\,\{2\cdot d\,(s_1) - 1, 2\cdot d\,(s_2)\} \\
\costconj{s}{g} & = & d\,(s) \\
s'_i & = & \mbox{the first state in the trace for $s$ after} \\
 & & \mbox{a transition delivering the answer $a_i$} \\
\end{array}
\]
\end{lemma}

For the states of the form $s \otimes g$ the reasoning is the same, but the resulting equations are more complicated.
In $\otimes$-state the left substate is evaluated until an answer is found, which is then taken as
\emph{an environment} for the evaluation of the right subgoal.
Thus, in the equations for $\otimes$-states the evaluation times of the second goal \emph{for all
the answers} generated for the first substate are summed up. The evaluation of the right subgoal
in different environments are added to the evaluation of the left substate via the creation of
an $\oplus$-state, so for scheduling factor there is
an additional summand $\costdisj{\taskst{g}{a_i}}{s'_i}$ for each answer with $s'_i$ being the state
after discovering the answer.
There is also an extra summand $\costconj{s}{g}$ for the scheduling factor because of the
$\otimes$-node that increases the height in the trace, analogous to the one caused by
$\oplus$-nodes.
Note, $\otimes$-node is always placed immediately over the left substate so this
addition is exactly the number of steps for the left substate.

Unfolding costs definitions in $(\dagger)$ gives us a cumbersome formula which 
includes all intermediate states encountered during the evaluation. However, as ultimately
we are interested in an asymptotic estimations, we can approximate these costs up to a multiplicative constant.

First, we rewrite the equation $(\dagger)$ in the following form:

\[ t\,(s \otimes g)  =  t\,(s) + \smashoperator[lr]{\sum\limits_{a_i \in \tra{s}}} t\,(\taskst{g}{a_i}) + C\,(s \otimes g) \]

where

\[ C\,(s \otimes g) = \costconj{s}{g} + \smashoperator[lr]{\sum\limits_{a_i \in \tra{s}}} \costdisj{\taskst{g}{a_i}}{(s'_i \otimes g)} \]

Next, we introduce the following definition.

\begin{definition}
Let $E$ be a set of environments, $g$~--- a goal. Then we call the value

\[
\alpha\,(g, E) = \argmax{e \in E} d\,(\taskst{g}{e})
\]

a \emph{pincipal environment}.
\end{definition}

In other words, $\alpha\,(g, E)$ is an element of $E$ which delivers the longest trace of $g$.

With principal environment being defined we can prove the following lemma\footnote{We assume the following definition for 
$h\,(x) = \Theta\,(f\,(x))$: \[\exists C_1, C_2 \in \mathcal{R^{+}}, \, \forall x : C_1 \cdot f\,(x) \le h\,(x) \le C_2 \cdot f\,(x) \]}:

\begin{lemma}

\[
C\,(s \otimes g) =
\Theta\,(d\,(s) + \smashoperator[lr]{\sum\limits_{\begin{array}{c}
                                                                                   \scalebox{0.7}{$a_i \in \tra{s}$} \\
                                                                                   \scalebox{0.7}{$a_i \ne \alpha\,(g, \tra{s}) $} \\
                                                                              \end{array}}} d\,(\taskst{g}{a_i}))	
\]
\end{lemma}

We can see that this approximation is very similar to the $(\star)$ except that here the principal environment is excluded. This difference
is essential and, as we will see later, it is in fact responsible for the difference in complexities for our motivation example.

Finally, as a result we have the following approximation for the scheduling factor of $\otimes$-states.

\begin{corollary}
\[
 t\,(s \otimes g)  =  t\,(s) + \left({\sum\limits_{a_i \in \tra{s}}} t\,(\taskst{g}{a_i})\right) +
 \Theta\,(d\,(s) + \smashoperator[lr]{\sum\limits_{\begin{array}{c}
                                                                                   \scalebox{0.7}{$a_i \in \tra{s}$} \\
                                                                                   \scalebox{0.7}{$a_i \ne \alpha\,(g, \tra{s}) $} \\
                                                                              \end{array}}} d\,(\taskst{g}{a_i}))	
\]
\end{corollary}









\begin{comment}

\textcolor{red}{\bf OR}

\begin{definition}
For two functions $f \colon X_1 \times \dots \times X_n \to \mathcal{R}$ and $g \colon X_1 \times \dots \times X_n \colon \mathcal{R} \to \mathcal{R}$ we say that $f(x_1,\, \dots,\, x_n) \sim_C g(x_1,\, \dots,\, x_n, C)$ if there exist two positive real constants $c^{low}$ and $c^{up}$ such that for all $(x_1,\, \dots,\, x_n) \in X_1 \times \dots \times X_n$, $g(x_1,\, \dots,\, x_n,\, c^{low}) \le f(x_1,\, \dots,\, x_n) \le g(x_1,\, \dots,\, x_n,\, c^{up})$.
\end{definition}

\begin{lemma}
\label{lem:prod_approximation}
For any state $s$ and any goal $g$
\[ 
\begin{array}{lcrl}

t\,(s \otimes g) & \lowupbound{1}{2} & & t\,(s) + (\displaystyle\sum\limits_{a_i \in \tra{s}} t\,(\taskst{g}{a_i})) + \\ 
& & C \cdot ( & d\,(s) + (\displaystyle\sum\limits_{a_i \in \tra{s}} d\,(\taskst{g}{a_i})) {\color{blue} - d\,(\taskst{g}{a_m})} ) 

\end{array} \]

where $a_m = \argmax{a_i \in \tra{s}} d(\taskst{g}{a_i})$
\end{lemma}

\textcolor{red}{\bf OR}

\begin{lemma}
\label{lem:prod_approximation}
\[ 
\begin{array}{lcrl}

t\,(s \otimes g) & \sim_C & & t\,(s) + (\displaystyle\sum\limits_{a_i \in \tra{s}} t\,(\taskst{g}{a_i})) + \\ 
& & C \cdot ( & d\,(s) + (\displaystyle\sum\limits_{a_i \in \tra{s}} d\,(\taskst{g}{a_i})) {\color{blue} - d\,(\taskst{g}{a_m})} ) 

\end{array} \]

where $a_m = \argmax{a_i \in \tra{s}} d(\taskst{g}{a_i})$
\end{lemma}

The blue addend is called ...

\end{comment}

\begin{comment}

One option is to go with the first argument of ``$\min$'' in $\costdisj{\taskst{g}{a_i}}{s'_i}$.
It should be a good approximation in the case when there are several answers passed to the second
goal and for none of them the number of steps surpasses the \emph{overall} number of steps for all
other answers (the second argument of ``$\min$'' will include the sum for the rest of the answers).

\begin{corollary}
\label{lem:prod_estimation_multiple_answers}
For any state $s$ and any goal $g$
\[ t\,(s \otimes g) \le t\,(s) + d\,(s) + \displaystyle\sum\limits_{a_i \in \tra{s}} (t\,(\taskst{g}{a_i}) + 2\cdot d\,(\taskst{g}{a_i}) \]
\end{corollary}

In the case when there is only one answer, however, we should rather go with the second argument of ``$\min$''. 

In this case the number of steps $d\,(s'_1 \otimes g)$ is equal to the number of steps $d\,(s'_1)$
since no more answers are produced, and we can approximate it by the length $d\,(s)$ of the whole
trace for $s$. 

\begin{corollary}
\label{lem:prod_estimation_single_answer}
  For any state $s$ and any goal $g$, if $\tra{s} = \{a\}$, then
  
\[ t\,(s \otimes g) \le t\,(s) + 3\cdot d\,(s) + t\,(\taskst{g}{a}) \]
\end{corollary}

Finally, since we estimate only up to a multiplicative constant (in particular, it does not matter by what constants we multiply the values of $d\,(\cdot)$ when calculating
the scheduling factor) we can derive from these results two compact scheduling time approximations for goals in the form of sequences of disjuncts/conjuncts.
These two approximations work regardless of the associativity/grouping of subformulas; thus a certain constant $c_k$, depending only on $k$, comes in.

For conjunctions, we have the following one.

\begin{lemma}
\label{lem:conjunction_metrics_calc}

Let $g = g_1 \land \dots \land g_k$ and let $A_i$ be a set of all answers that are passed to $g_i$ at some stage starting from some initial environment $e_0$

\[
\begin{array}{rcl}
A_1 &=& \{ e_0 \} \\
A_{i + 1} & = & \bigcup\limits_{a \in A_i} \tra{\taskst{g_i}{a}} 
\end{array}
\]

Then

\[
\begin{array}{rcl}
d\,(\taskst{g}{e}) &=& \displaystyle\sum\limits_{1 \le i \le k} \;\; \displaystyle\sum\limits_{a \in A_i} d\,(\taskst{g_i}{a}) \\
t\,(\taskst{g}{e}) &\le& \displaystyle\sum\limits_{1 \le i \le k} \;\; \displaystyle\sum\limits_{a \in A_i} t\,(\taskst{g_i}{a}) + c_k \cdot \displaystyle\sum\limits_{1 \le i \le k} \;\; \displaystyle\sum\limits_{a \in A_i} d\,(\taskst{g_i}{a}), \\
\end{array}
\]

In the case when all $A_i$ contain only one answer

\[
t\,(\taskst{g}{e}) \le \displaystyle\sum\limits_{1 \le i \le k} \;\; \displaystyle\sum\limits_{a \in A_i} t\,(\taskst{g_i}{a}) + c_k \cdot \displaystyle \sum\limits_{1 \le i \le k - 1} \;\; \displaystyle\sum\limits_{a \in A_i} d\,(\taskst{g_i}{a})
\]

\end{lemma}

When applying the estimation from corollary~\ref{lem:prod_estimation_multiple_answers} we have an extra summand in the form of the number of steps (multiplied by some constant) for all conjuncts.
The only exception is the case when every conjunct produces no more than one answer, then we can use the corollary~\ref{lem:prod_estimation_single_answer} everywhere instead and drop out the
additional number of steps for the last conjunct. Besides that, a constant number of steps is required to turn each conjunction into a $\otimes$-state, but we may integrate this extra constant into $c_k$.

For disjunctions, the lemma is the following one.

\begin{lemma}
\label{lem:disjunction_metrics_calc}

Let $g = g_1 \lor \dots \lor g_k$ and $1 \le l \le k$; then

\[
\renewcommand{\arraystretch}{1.5}
\begin{array}{rcl}
  d\,(\taskst{g}{e}) &\le& \displaystyle\sum\limits_{1 \le i \le k} d\,(\taskst{g_i}{e}) \\
  t\,(\taskst{g}{e}) &\le& \displaystyle\sum\limits_{1 \le i \le k} t\,(\taskst{g_i}{e}) + c_k\cdot \displaystyle\sum\limits_{\renewcommand{\arraystretch}{1}\begin{array}{c}1 \le i \le k \\ i \ne l\end{array}} d\,(\taskst{g_i}{e}).
\end{array}
\]

\end{lemma}

Roughly speaking, if we have a disjunct $g_m$ with a number of steps larger than all the steps for other disjuncts combined, then when applying lemma~\ref{lem:sum_estimation} we again will have an
extra summand in the form of the number of steps for all disjuncts except for the $g_m$ (it will always be the largest argument of ``$\min$''). But if we can drop out the \emph{largest}
the number of steps among disjuncts, we can also drop out any other instead, that's where arbitrary $l$ comes from. The case when there is no such $g_m$ has to be considered separately; it is simpler
since then all the numbers of steps are the same up to a multiplicative constant.

\end{comment}
